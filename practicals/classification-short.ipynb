{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KCL-Health-NLP/nlp-half-day-workshop/blob/main/practicals/classification-short.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QSLVKYpFLsr"
      },
      "source": [
        "# NLP classification - supervised learning\n",
        "## A short example\n",
        "\n",
        "In this example, you will learn how you can use supervised learning algorithms for NLP classification. We will use documents from [MTSamples](http://www.mtsamples.com/). These are transcribed sample medical reports and examples from a variety of clinical disciplines, such as pediatrics, haematology, radiology, surgery, discharge summaries. Note that one document can belong to several categories.\n",
        "\n",
        "The task is to classify a document into its clinical specialty, e.g. pediatrics or hematology.\n",
        "\n",
        "We will use the simple K Nearest Neighbours classification algorithms as implemented in a popular Python machine learning library, [scikit-learn](https://scikit-learn.org/stable/), and evaluate with cross-validation before testing on unseen test data. We will use the [pandas](https://pandas.pydata.org/) library to store and handle our data.\n",
        "\n",
        "We will experiment with a couple of different ways of representing the documents for the classifiers.\n",
        "\n",
        "Material in parts from https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f\n",
        "\n",
        "Written by Sumithra Velupillai, March 2019 - adapted and updated February 2020, April 2024, January 2025 by Sumithra Velupillai and Angus Roberts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAf7_A_FFLss"
      },
      "source": [
        "# 1: Packages\n",
        "We will use a number of different packages for this exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9dsDjZ3FLst"
      },
      "outputs": [],
      "source": [
        "# We will use matplotlib to graph our results\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# And pandas to store our data\n",
        "import pandas as pd\n",
        "\n",
        "# numpy for number and vector handling\n",
        "import numpy as np\n",
        "\n",
        "import warnings; warnings.simplefilter('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_on5sqPFFLsu"
      },
      "outputs": [],
      "source": [
        "# We'll use scikit-learn for the classification algorithms.\n",
        "# https://scikit-learn.org/stable/\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L3YE4vzFLsu"
      },
      "outputs": [],
      "source": [
        "## sklearn also has some nice funtions for representations\n",
        "\n",
        "# Bag-of-words implementation in sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# TfIdf implementation in sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "## and for evaluation\n",
        "# from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODSBKIZ-FLsu"
      },
      "outputs": [],
      "source": [
        "## Since we're working with text, we might need to tokenize for some of these representations.\n",
        "# We'll use nltk here, but there are other nlp packages available for this\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKc8HrRvFLsv"
      },
      "source": [
        "# 2: Corpus\n",
        "Read in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao--xPOsFLsv"
      },
      "outputs": [],
      "source": [
        "# Copy files from github in to the local Colab filespace.\n",
        "!git clone --quiet https://github.com/KCL-Health-NLP/nlp-half-day-workshop.git\n",
        "print(\"Done copying files\")\n",
        "\n",
        "xlds_training = './nlp-half-day-workshop/practicals/classification_training_data.xlsx'\n",
        "trainingdata = pd.read_excel(xlds_training)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymTrWbm6FLsv"
      },
      "source": [
        "Take a look at the content of the training data. What are we trying to classify? What are the labels we want to try to learn? How many instances do we have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4WkM3l5FLsv"
      },
      "outputs": [],
      "source": [
        "trainingdata['label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_C5XbyDFLsw"
      },
      "source": [
        "What types of features do you think would be useful for the classification task? Where can we get them? Take a look at one or two of the documents. Can you guess which classification label these belong to?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c-Oq-KRFLsw"
      },
      "outputs": [],
      "source": [
        "trainingtxt_example = trainingdata['txt'].tolist()[0]\n",
        "print(trainingtxt_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MExLPegGFLsw"
      },
      "outputs": [],
      "source": [
        "trainingtxt_example = trainingdata['txt'].tolist()[231]\n",
        "print(trainingtxt_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "646MrP1BFLsw"
      },
      "source": [
        "# 3: Representation - BoW\n",
        "\n",
        "The most common baseline feature representation for text classification tasks is to use the *bag-of-words* representation, in a document-term matrix. Let's build a simple one using raw counts and only keeping a maximum of 500 features. We can use the CountVectorizer function from sklearn, and tokenize using a function from nltk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BobArOMLFLsw"
      },
      "outputs": [],
      "source": [
        "first_vectorizer = CountVectorizer(ngram_range=(1,1), stop_words=None,\n",
        "                             tokenizer=word_tokenize, max_features=500)\n",
        "first_vectorizer.fit(trainingdata['txt'].tolist())\n",
        "first_fit_transformed_data = first_vectorizer.fit_transform(trainingdata['txt'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SSqN4gEFLsw"
      },
      "source": [
        "We can now look at this transformed representation for an example document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TOZgaNdFLsx"
      },
      "outputs": [],
      "source": [
        "first_transformed_data = first_vectorizer.transform([trainingdata['txt'].tolist()[231]])\n",
        "print (first_transformed_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQl9mGL7FLsx"
      },
      "source": [
        "What word is represented by the different indices? Have a look at a few examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vN1lywr9FLsx"
      },
      "outputs": [],
      "source": [
        "print (first_vectorizer.get_feature_names_out()[32])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB428IjSFLsx"
      },
      "outputs": [],
      "source": [
        "print(first_fit_transformed_data.shape)\n",
        "print ('Amount of Non-Zero occurences: ', first_fit_transformed_data.nnz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udtv8IMpFLsx"
      },
      "source": [
        "# 4: Classification\n",
        "Let's build a classifier with this feature representation. In text classification, many classification algorithms have been shown to work well. Sci-kit learn has implementations for many different types of classification algorithms - have a look at their website!\n",
        "\n",
        "Let's try a K nearest neighbour classifier. This builds a model that assigns classes to test examples based on the majority class of the nerest k training examples to that test example. By default, Sci-kit Learn's KNN classifier will look at the closest 5 neighbours.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky4UmzoVFLsx"
      },
      "outputs": [],
      "source": [
        "kneighbour_classifier = KNeighborsClassifier().fit(first_fit_transformed_data, trainingdata['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j319_tJcFLsx"
      },
      "source": [
        "We now have a trained model. But how do we know how well it works? Let's evaluate it on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0wEaxcBFLsx"
      },
      "outputs": [],
      "source": [
        "\n",
        "xlds_test = './nlp-half-day-workshop/practicals/classification_test_data.xlsx'\n",
        "testdata = pd.read_excel(xlds_test)\n",
        "\n",
        "\n",
        "\n",
        "## We need to transform this data to the same representation\n",
        "first_fit_transformed_testdata = first_vectorizer.transform(testdata['txt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTcrDtSZFLsy"
      },
      "outputs": [],
      "source": [
        "first_fit_transformed_testdata\n",
        "kneighbour_predicted = kneighbour_classifier.predict(first_fit_transformed_testdata)\n",
        "kneighbour_predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make a list of all the labels in our dataset to evaluate, and then run some standard evaluation metrics"
      ],
      "metadata": {
        "id": "mRGcsTkrjd5w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLXSlVpnFLsy"
      },
      "outputs": [],
      "source": [
        "labels = list(set(testdata['label']))\n",
        "print(metrics.classification_report(testdata['label'], kneighbour_predicted, target_names=labels))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPyd1hxdFLsy"
      },
      "source": [
        "What do you think about these results? There are probably ways of improving this, by changing the representation or maybe trying a different classifier model.\n",
        "__There is one main problem though: we can't use this test data to try different configurations! Why?__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M4_bqpIFLsy"
      },
      "source": [
        "# 5: N-fold cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ-o8TbrFLsy"
      },
      "source": [
        "We can employ n-fold cross-validation on the training data to experiment with different representations, parameters, and classifiers.\n",
        "\n",
        "There are also various metrics that can be used to evaluate classification results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2akkMilFLsy"
      },
      "outputs": [],
      "source": [
        "kneighbour_classifier = KNeighborsClassifier().fit(first_fit_transformed_data, trainingdata['label'])\n",
        "scoring = ['precision_macro', 'recall_macro','precision_micro','recall_micro', 'f1_micro', 'f1_macro']\n",
        "scores = cross_validate(kneighbour_classifier, first_fit_transformed_data, trainingdata['label'], scoring=scoring, cv=10, return_train_score=False)\n",
        "scoresdf = pd.DataFrame(scores)\n",
        "scoring = ['test_precision_macro', 'test_recall_macro','test_precision_micro','test_recall_micro', 'test_f1_micro', 'test_f1_macro']\n",
        "bp = scoresdf.boxplot(column=scoring, grid=False, rot=45,)\n",
        "[ax_tmp.set_xlabel('') for ax_tmp in np.asarray(bp).reshape(-1)]\n",
        "fig = np.asarray(bp).reshape(-1)[0].get_figure()\n",
        "fig.suptitle('K nearest neighbour, count vectorizer')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq_j0UgjFLsz"
      },
      "source": [
        "# 6: Another representation model: Tf-idf\n",
        "We have used a very simple bag-of-words representation. What happens if we try something else? Let's try a representaiton called tf-idf - Term Frequency, Inverse Document Frequency. Tf-idf is a word frequency model like bag-of-words, but it adjusts frequencies to take in to account how rare words are. A rare word might be expected to help us distinguish between documents. So a very common word is given less weight than a rare word.  Tf-idf is considered a strong baseline in many text classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6Pi1snbFLsz"
      },
      "outputs": [],
      "source": [
        "\n",
        "stopWords = list(stopwords.words('english'))\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopWords)\n",
        "tfidf_vect.fit(trainingdata['txt'])\n",
        "second_fit_transformed_data =  tfidf_vect.transform(trainingdata['txt'])\n",
        "second_fit_transformed_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsE-xQ7jFLsz"
      },
      "source": [
        "What other parameters can you change in this representation? How does this look different from the CountVectorizer representation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28a8YRkuFLsz"
      },
      "source": [
        "Let's now use this with the KNN classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92RB9RcfFLsz"
      },
      "outputs": [],
      "source": [
        "kneighbour_classifier = KNeighborsClassifier().fit(second_fit_transformed_data, trainingdata['label'])\n",
        "scoring = ['precision_macro', 'recall_macro','precision_micro','recall_micro', 'f1_micro', 'f1_macro']\n",
        "scores = cross_validate(kneighbour_classifier, second_fit_transformed_data, trainingdata['label'], scoring=scoring, cv=10, return_train_score=False)\n",
        "scoresdf = pd.DataFrame(scores)\n",
        "scoring = ['test_precision_macro', 'test_recall_macro','test_precision_micro','test_recall_micro', 'test_f1_micro', 'test_f1_macro']\n",
        "bp = scoresdf.boxplot(column=scoring, grid=False, rot=45,)\n",
        "[ax_tmp.set_xlabel('') for ax_tmp in np.asarray(bp).reshape(-1)]\n",
        "fig = np.asarray(bp).reshape(-1)[0].get_figure()\n",
        "fig.suptitle('K nearest neighbour, tf-idf vectorizer')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhWZKYrTFLsz"
      },
      "source": [
        "This looks better, doesn't it? Why do you think this works better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbYUZZWTFLs3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}